Ollama
======

Ollama 是一款优秀的开源工具，让您可以在本地轻松运行和管理各种大型语言模型（LLMs）。Cherry Studio 现已支持 Ollama 集成，让您可以在熟悉的界面中，直接与本地部署的 LLM 进行交互，无需依赖云端服务！

[hashtag](#shen-me-shi-ollama)

什么是 Ollama？

-----------------------------------------------

Ollama 是一个简化大型语言模型（LLM）部署和使用的工具。它具有以下特点：

*   **本地运行：** 模型完全在您的本地计算机上运行，无需联网，保护您的隐私和数据安全。
    
*   **简单易用：** 通过简单的命令行指令，即可下载、运行和管理各种 LLM。
    
*   **模型丰富：** 支持 Llama 2、Deepseek、Mistral、Gemma 等多种流行的开源模型。
    
*   **跨平台：** 支持 macOS、Windows 和 Linux 系统。
    
*   **开放API**：支持与OpenAI兼容的接口，可以和其他工具集成。
    

[hashtag](#wei-shen-me-yao-zai-cherry-studio-zhong-shi-yong-ollama)

为什么要在 Cherry Studio 中使用 Ollama？

--------------------------------------------------------------------------------------------------------

*   **无需云服务：** 不再受限于云端 API 的配额和费用，尽情体验本地 LLM 的强大功能。
    
*   **数据隐私：** 您的所有对话数据都保留在本地，无需担心隐私泄露。
    
*   **离线可用：** 即使在没有网络连接的情况下，也能继续与 LLM 进行交互。
    
*   **定制化：** 可以根据您的需求，选择和配置最适合您的 LLM。
    

[hashtag](#zai-cherry-studio-zhong-pei-zhi-ollama)

在 Cherry Studio 中配置 Ollama

----------------------------------------------------------------------------------

### 

[hashtag](#id-1.-an-zhuang-he-yun-xing-ollama)

**1\. 安装和运行 Ollama**

首先，您需要在您的计算机上安装并运行 Ollama。请按照以下步骤操作：

*   **下载 Ollama：** 访问 Ollama 官网（[https://ollama.com/](https://ollama.com/)），根据您的操作系统下载对应的安装包。 在 Linux 下，可直接运行命令安装ollama：
    
*   **安装 Ollama：** 按照安装程序的指引完成安装。
    
*   **下载模型：** 打开终端（或命令提示符），使用 `ollama run` 命令下载您想要使用的模型。例如，要下载 Llama 2 模型，可以运行：
    
    Ollama 会自动下载并运行该模型。
    
*   **保持 Ollama 运行：** 在您使用 Cherry Studio 与 Ollama 模型交互期间，请确保 Ollama 保持运行状态。
    

### 

[hashtag](#id-2.-zai-cherry-studio-zhong-tian-jia-ollama-fu-wu-shang)

**2\. 在 Cherry Studio 中添加 Ollama 服务商**

接下来，在 Cherry Studio 中添加 Ollama 作为自定义 AI 服务商：

*   **打开设置：** 在 Cherry Studio 界面左侧导航栏中，点击“设置”（齿轮图标）。
    
*   **进入模型服务：** 在设置页面中，选择“模型服务”选项卡。
    
*   **添加提供商：** 点击列表中的 Ollama。
    

![](https://docs.cherry-ai.com/~gitbook/image?url=https%3A%2F%2F3562065924-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252F0Ut5BptC3t8CtSU1UWpM%252Fuploads%252Fgit-blob-2649f1f08ffc8257ebf381425a595ac5e04e4848%252Fimage%2520%285%29%2520%283%29.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2b764f94&sv=2)

### 

[hashtag](#id-3.-pei-zhi-ollama-fu-wu-shang)

**3\. 配置 Ollama 服务商**

在服务商列表中找到刚刚添加的 Ollama，并进行详细配置：

1.  **启用状态：**
    
    *   确保 Ollama 服务商最右侧的开关已打开，表示已启用。
        
    
2.  **API 密钥：**
    
    *   Ollama 默认**不需要** API 密钥。您可以将此字段留空，或者填写任意内容。
        
    
3.  **API 地址：**
    
    *   填写 Ollama 提供的本地 API 地址。通常情况下，地址为：
        
        如果修改了端口，请自行更改。
        
    
4.  **保持活跃时间：** 此选项是设置会话的保持时间，单位是分钟。如果在设定时间内没有新的对话，Cherry Studio 会自动断开与 Ollama 的连接，释放资源。
    
5.  **模型管理：**
    
    *   点击“+ 添加”按钮，手动添加您在 Ollama 中已经下载的模型名称。
        
    *   比如您已经通过`ollama run llama3.2`下载了`llama3.2`模型, 那么此处可以填入`llama3.2`
        
    *   点击“管理”按钮，可以对已添加的模型进行编辑或删除。
        
    

[hashtag](#kai-shi-shi-yong)

开始使用

--------------------------------------

完成以上配置后，您就可以在 Cherry Studio 的聊天界面中，选择 Ollama 服务商和您已下载的模型，开始与本地 LLM 进行对话了！

[hashtag](#ji-qiao-yu-ti-shi)

技巧与提示

----------------------------------------

*   **首次运行模型：** 第一次运行某个模型时，Ollama 需要下载模型文件，可能需要较长时间，请耐心等待。
    
*   **查看可用模型：** 在终端中运行 `ollama list` 命令，可以查看您已下载的 Ollama 模型列表。
    
*   **硬件要求：** 运行大型语言模型需要一定的计算资源（CPU、内存、GPU），请确保您的计算机配置满足模型的要求。
    
*   **Ollama 文档**: 可以点击配置页面中的`查看Ollama文档和模型`链接快速跳转至Ollama官网文档。
    

[上一页OneAPI](/pre-basic/providers/oneapi)[下一页GitHub Copilot](/pre-basic/providers/github-copilot)

最后更新于10个月前

*   [什么是 Ollama？](#shen-me-shi-ollama)
*   [为什么要在 Cherry Studio 中使用 Ollama？](#wei-shen-me-yao-zai-cherry-studio-zhong-shi-yong-ollama)
*   [在 Cherry Studio 中配置 Ollama](#zai-cherry-studio-zhong-pei-zhi-ollama)
*   [1\. 安装和运行 Ollama](#id-1.-an-zhuang-he-yun-xing-ollama)
*   [2\. 在 Cherry Studio 中添加 Ollama 服务商](#id-2.-zai-cherry-studio-zhong-tian-jia-ollama-fu-wu-shang)
*   [3\. 配置 Ollama 服务商](#id-3.-pei-zhi-ollama-fu-wu-shang)
*   [开始使用](#kai-shi-shi-yong)
*   [技巧与提示](#ji-qiao-yu-ti-shi)

$RB=\[\];$RV=function(b){$RT=performance.now();for(var a=0;a<b.length;a+=2){var c=b\[a\],e=b\[a+1\];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g.\_reactRetry&&g.\_reactRetry()}}b.length=0}; $RC=function(b,a){if(a=document.getElementById(a))(b=document.getElementById(b))?(b.previousSibling.data="$~",$RB.push(b,a),2===$RB.length&&(b="number"!==typeof $RT?0:$RT,a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:b+300-a))):a.parentNode.removeChild(a)};

复制

    curl -fsSL https://ollama.com/install.sh | sh

复制

    ollama run llama3.2

复制

    http://localhost:11434/
